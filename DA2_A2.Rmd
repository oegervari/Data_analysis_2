---
title: "DA2 - Assignment 2"
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE, warning=F}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
# Set graph size
knitr::opts_chunk$set(echo = FALSE, out.width = "50%", fig.asp = 0.5, fig.width = 7, out.width = "90%" )

rm(list=ls())

# Libraries
library(tidyverse)
library(modelsummary)
library(fixest)
library(ggpubr)
library(lspline)
library(mfx)

# Get the data
hotels_europe_price <- read_csv("https://osf.io/p6tyr/download")
hotels_europe_features <- read_csv("https://osf.io/utwjs/download")

data <- left_join(hotels_europe_price, hotels_europe_features, by = "hotel_id")
rm(hotels_europe_price,hotels_europe_features)

```

## Introduction

In this analysis I investigate whether there is a relationship between hotels' (high) rating and their stars and distances to the city center. I used the [hotel-europe dataset](https://osf.io/p6tyr/) with the location set to Berlin. 


```{r, echo=FALSE}
# Sample selection
data <- data %>% filter(city_actual == 'Berlin')
data$highly_rated <- ifelse(data$rating >= 4, 1, 0)

asd <- summary(data$highly_rated)  #  mean is 0.6, means that 60% of hotels are highly rated
```

I created a binary variable based on the hotel ratings, if a hotel is rated 4 or above, the highly_rated column takes a value of 1, otherwise 0. Based on this, the mean value of this variable for the dataset is `r round(asd[4],3)`. That is also the probability of a hotel being highly rated in Berlin.

As the next step, I calculated the predictions of high rating (of hotels) using the linear probability, logit and probit models and plotted the results in the following graph.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center", message=F }
model_formula <- formula( highly_rated ~ distance + stars )

lpm <- feols(model_formula, data = data, vcov = 'hetero')

pred_lpm <- predict(lpm)

#logit coefficients
logit <- feglm(model_formula, data = data, family = binomial( link = "logit" ))

# predicted logit probabilities
pred_logit <- predict(logit)

#Logit marginal differences
logit_marg <- logitmfx( model_formula, data=data, atmean=FALSE, robust = T )


#probit coefficients
probit <- feglm(model_formula, data = data, family = binomial( link = "probit" ))

# predicted probit probabilities
pred_probit <- predict(probit)

#Probit marginal differences
probit_marg <- probitmfx( model_formula, data=data, atmean=FALSE, robust = T )

# Comparing predicted probabilities of logit and probit to LPM
ggplot(data = data[1:4200,]) +
  geom_point(aes(x=pred_lpm, y=pred_probit, color="Probit"), size=0.7,  shape=16) +
  geom_point(aes(x=pred_lpm, y=pred_logit,  color="Logit"), size=0.7,  shape=16) +
  geom_line(aes(x=pred_lpm, y=pred_lpm,    color="45 degree line"), size=0.7) +
  labs(x = "Predicted probability of high rating (LPM)", y="Predicted probability")+
  scale_y_continuous(expand = c(0.00,0.0), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_x_continuous(expand = c(0.00,0.0), limits = c(0,1), breaks = seq(0,1,0.1))

```

The 45 degree line is the linear probability model, the two S-curves are the logit and probit models. Based on the eye-test, the results look fairly similar, but let's take a look at the coefficient results in the next chart. 

```{r, echo=FALSE, warning=FALSE, fig.width=4, fig.height = 3, fig.align="center" }
# Including the marginals:
cm <- c('(Intercept)' = 'Constant')
pmodels <- list('LPM'=lpm, 'logit coeffs'=logit, 'logit marginals'=logit_marg, 'probit coeffs'=probit, 'probit marginals'=probit_marg)

msummary( pmodels ,
          fmt="%.3f",
          gof_omit = 'DF|Deviance|Log.Lik.|F|R2 Adj.|AIC|BIC|R2|PseudoR2',
          stars=c('*' = .05, '**' = .01),
          coef_rename = cm)

```
In case of logit and probit models, instead of the raw coefficients, we use the marginal differences, which have the same interpretation as the coefficients in case of linear probability models. 
Based on this, the three models produce very similar results.
If we look at the logit marginals we can see, that if the distance is greater, we can expect the probability of high rating to be lower by `r round(logit_marg$mfxest[1]*-100, 2)`%. And if a hotel possesses more stars, the probability of it being highly rated is greater by `r round(logit_marg$mfxest[2]*100, 2)`%.

```{r, echo=FALSE}
fitstat_register("brier", function(x){mean(x$residual^2)}, "Brier score")
dsa <- etable( lpm, logit, probit , drop = "factor|lspline|income|exerc",fitstat = ~ brier  )
```

In terms of goodness of fit, based on the Brier-score, the LPM model provides the best prediction with a value of `r round(as.numeric(dsa$lpm[9]), 2)`. The whole table can be found in the appendix.


## Conclusion

We can conclude, that not surprisingly the farther the hotel from the city center, the lower the possibility of a high rating. Even less surprising, that the higher the number of a stars, the higher the possibility of a high rating. It should be noted, that probably in Berlin, where there is no definite city center, like in Budapest for example, the so called distance variable might not have a strong relationship with the probability of high rating, since the people, who visit Berlin, usually have different preferences in terms of choosing a location for accommodation.

## Appendix

Summary table on the 'highly_rated' variable

```{r, echo=FALSE}
# Sample selection
asd
```
Linear probability model

```{r, echo=FALSE}
# Sample selection
lpm
```

Logit model

```{r, echo=FALSE}
# Sample selection
logit
```

Probit model

```{r, echo=FALSE}
# Sample selection
probit
```

Logit marginal differences 

```{r, echo=FALSE}
# Sample selection
logit_marg
```

Probit marginal differences 

```{r, echo=FALSE}
# Sample selection
probit_marg
```

Brier-score

```{r, echo=FALSE}
dsa
```








